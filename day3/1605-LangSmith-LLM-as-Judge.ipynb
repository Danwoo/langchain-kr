{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a02867dd",
   "metadata": {},
   "source": [
    "# LLM-as-Judge\n",
    "\n",
    "LangSmith 에서 제공되는 Off-the-shelf Evaluators 를 활용해 보겠습니다.\n",
    "\n",
    "Off-the-shelf Evaluators 는 사전에 정의된 프롬프트 기반의 LLM 평가자를 의미합니다.\n",
    "\n",
    "쉽게 사용할 수 있는 이점이 있지만, 더 확장된 기능을 사용하기 위해서는 직접 평가자를 정의해야 합니다.\n",
    "\n",
    "기본적으로 다음의 3가지 정보를 LLM Evaluator 에 전달하여 평가를 진행합니다.\n",
    "\n",
    "- `input`: 질문. 보통 데이터셋의 Question 이 사용됩니다.\n",
    "- `prediction`: LLM 이 생성한 답변. 보통 모델의 답변이 사용됩니다.\n",
    "- `reference`: 정답 답변, Context 등 변칙적으로 활용이 가능.\n",
    "\n",
    "**참고**\n",
    "- https://docs.smith.langchain.com/evaluation/faq/evaluator-implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a30a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설치\n",
    "# !pip install -qU langsmith langchain-teddynote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d75d1492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633d9db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install -qU langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"CH16-Evaluations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09c8411",
   "metadata": {},
   "source": [
    "## RAG 성능 테스트를 위한 함수 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011f17eb",
   "metadata": {},
   "source": [
    "테스트에 활용할 RAG 시스템을 생성하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a79916b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': '삼성전자가 자체 개발한 생성형 AI의 이름은 무엇인가요?',\n",
       " 'result': \"삼성전자가 자체 개발한 생성형 AI의 이름은 '삼성 가우스'입니다.\"}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 표준 LangChain 기반 RAG 예제 (myrag 모듈 없이 동작)\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# PDF 문서 로드\n",
    "loader = PyPDFLoader(\"data/SPRI_AI_Brief_2023년12월호_F.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# 텍스트 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# 임베딩 및 벡터스토어 생성\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# QA 체인 생성\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "\n",
    "# 질문에 대한 답변 생성\n",
    "qa_chain.invoke({\"query\": \"삼성전자가 자체 개발한 생성형 AI의 이름은 무엇인가요?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b71e6e",
   "metadata": {},
   "source": [
    "`ask_question` 이라는 이름으로 함수를 생성합니다. 입력으로는 `inputs` 라는 딕셔너리를 받고, 출력으로는 `answer` 라는 딕셔너리를 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a922c6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문에 대한 답변하는 함수를 생성\n",
    "# qa_chain을 사용하여 답변 생성\n",
    "\n",
    "def ask_question(inputs: dict):\n",
    "    return {\"answer\": qa_chain.invoke({\"query\": inputs[\"question\"]})}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39bcf1e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': {'query': '삼성전자가 자체 개발한 생성형 AI의 이름은 무엇인가요?',\n",
       "  'result': \"삼성전자가 자체 개발한 생성형 AI의 이름은 '삼성 가우스'입니다.\"}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사용자 질문 예시\n",
    "llm_answer = ask_question(\n",
    "    {\"question\": \"삼성전자가 자체 개발한 생성형 AI의 이름은 무엇인가요?\"}\n",
    ")\n",
    "llm_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ad180e",
   "metadata": {},
   "source": [
    "evaluator prompt 출력을 위한 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c8799bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator prompt 출력을 위한 함수\n",
    "def print_evaluator_prompt(evaluator):\n",
    "    return evaluator.evaluator.prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b281e209",
   "metadata": {},
   "source": [
    "## Question-Answer Evaluator\n",
    "\n",
    "가장 기본 기능을 가진 Evaluator 입니다. 질문(Query) 와 답변(Answer) 을 평가합니다.\n",
    "\n",
    "사용자 입력은 `input` 으로 LLM 이 생성한 답변은 `prediction` 으로 정답 답변은 `reference` 로 정의됩니다.\n",
    "\n",
    "(하지만, Prompt 변수는 `query`, `result`, `answer` 로 정의됩니다.)\n",
    "\n",
    "- `query`: 질문\n",
    "- `result`: LLM 답변\n",
    "- `answer`: 정답 답변"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61eaae63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a teacher grading a quiz.\n",
      "You are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\n",
      "\n",
      "Example Format:\n",
      "QUESTION: question here\n",
      "STUDENT ANSWER: student's answer here\n",
      "TRUE ANSWER: true answer here\n",
      "GRADE: CORRECT or INCORRECT here\n",
      "\n",
      "Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \n",
      "\n",
      "QUESTION: \u001b[33;1m\u001b[1;3m{query}\u001b[0m\n",
      "STUDENT ANSWER: \u001b[33;1m\u001b[1;3m{result}\u001b[0m\n",
      "TRUE ANSWER: \u001b[33;1m\u001b[1;3m{answer}\u001b[0m\n",
      "GRADE:\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "# qa 평가자 생성\n",
    "qa_evalulator = LangChainStringEvaluator(\"qa\")\n",
    "\n",
    "# 프롬프트 출력\n",
    "print_evaluator_prompt(qa_evalulator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ea33ce",
   "metadata": {},
   "source": [
    "평가를 진행하고, 출력된 URL 로 이동하여 결과를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07be19cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\git\\langchain-kr\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'RAG_EVAL-fb05cd27' at:\n",
      "https://smith.langchain.com/o/d898bfb2-ece6-4cd6-ac84-3a348a8c5546/datasets/c60820b3-16d2-4901-8dc1-fbde819f2286/compare?selectedSessions=ba66c8a1-fd9f-49ff-97f0-52f0168fb115\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:07,  1.41s/it]\n",
      "5it [00:07,  1.41s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"RAG_EVAL_DATASET\"\n",
    "\n",
    "# 평가 실행\n",
    "experiment_results = evaluate(\n",
    "    ask_question,\n",
    "    data=dataset_name,\n",
    "    evaluators=[qa_evalulator],\n",
    "    experiment_prefix=\"RAG_EVAL\",\n",
    "    # 실험 메타데이터 지정\n",
    "    metadata={\n",
    "        \"variant\": \"QA Evaluator 를 활용한 평가\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab107895",
   "metadata": {},
   "source": [
    "![](./assets/output-01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7b726d",
   "metadata": {},
   "source": [
    "## Context 에 기반한 답변 Evaluator\n",
    "\n",
    "- `LangChainStringEvaluator(\"context_qa\")`: LLM 체인에 정확성을 판단하는 데 참조 \"context\" 를 사용하도록 지시합니다.\n",
    "- `LangChainStringEvaluator(\"cot_qa\")`: `\"cot_qa\"` 는 `\"context_qa\"` 평가자와 유사하지만, 최종 판결을 결정하기 전에 LLM 의 '추론'을 사용하도록 지시한다는 점에서 차이가 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4261642a",
   "metadata": {},
   "source": [
    "**참고**\n",
    "\n",
    "먼저, Context 를 반환하는 함수를 정의해야 합니다: `context_answer_rag_answer`\n",
    "\n",
    "그 다음, `LangChainStringEvaluator` 를 생성합니다. 생성시 `prepare_data` 를 통해 위에서 정의한 함수의 반환 값을 적절하게 매핑합니다.\n",
    "\n",
    "**세부사항**\n",
    "\n",
    "- `run`: LLM 이 생성한 결과 (`context`, `answer`, `input`)\n",
    "- `example`: 데이터셋에 정의된 데이터입니다. (`question` 과 `answer`)\n",
    "\n",
    "`LangChainStringEvaluator` 이 평가를 수행하기 위하여 다음의 3가지 정보가 필요합니다.\n",
    "\n",
    "- `prediction`: LLM 이 생성한 답변\n",
    "- `reference`: 데이터셋에 정의된 답변\n",
    "- `input`: 데이터셋에 정의된 질문\n",
    "\n",
    "하지만, `LangChainStringEvaluator(\"context_qa\")` 는 `reference` 를 Context 로 사용하기 때문에 다음과 같이 정의합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db10c25",
   "metadata": {},
   "source": [
    "(참고) 아래는 `context_qa` 평가자를 활용하기 위하여 `context`, `answer`, `question` 을 반환하는 함수를 정의하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53b7f804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context 를 반환하는 RAG 결과 반환 함수\n",
    "# qa_chain을 사용하여 답변 생성\n",
    "\n",
    "def context_answer_rag_answer(inputs: dict):\n",
    "    context = retriever.invoke(inputs[\"question\"])\n",
    "    return {\n",
    "        \"context\": \"\\n\".join([doc.page_content for doc in context]),\n",
    "        \"answer\": qa_chain.invoke({\"query\": inputs[\"question\"]}),\n",
    "        \"query\": inputs[\"question\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cb38653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': 'SPRi AI Brief |  \\n2023-12 월호\\n10삼성전자 , 자체 개발 생성 AI ‘삼성 가우스 ’ 공개\\nn삼성전자가 온디바이스에서 작동 가능하며 언어, 코드, 이미지의 3개 모델로 구성된 자체 개발 생성 \\nAI 모델 ‘삼성 가우스 ’를 공개\\nn삼성전자는 삼성 가우스를 다양한 제품에 단계적으로 탑재할 계획으로 , 온디바이스 작동이 가능한 \\n삼성 가우스는 외부로 사용자 정보가 유출될 위험이 없다는 장점을 보유KEY Contents\\n£언어, 코드, 이미지의 3개 모델로 구성된 삼성 가우스 , 온디바이스 작동 지원\\nn삼성전자가 2023 년 11월 8일 열린 ‘삼성 AI 포럼 2023’ 행사에서 자체 개발한 생성 AI 모델 \\n‘삼성 가우스 ’를 최초 공개\\n∙정규분포 이론을 정립한 천재 수학자 가우스 (Gauss) 의 이름을 본뜬 삼성 가우스는 다양한 상황에 \\n최적화된 크기의 모델 선택이 가능\\n∙삼성 가우스는 라이선스나 개인정보를 침해하지 않는 안전한 데이터를 통해 학습되었으며 ,\\n사내 소프트웨어 개발에 최적화\\n∙이미지 모델은 창의적인 이미지를 생성하고 기존 이미지를 원하는 대로 바꿀 수 있도록 지원하며 \\n저해상도 이미지의 고해상도 전환도 지원\\nnIT 전문지 테크리퍼블릭 (TechRepublic) 은 온디바이스 AI가 주요 기술 트렌드로 부상했다며 , \\n2024 년부터 가우스를 탑재한 삼성 스마트폰이 메타의 라마(Llama)2 를 탑재한 퀄컴 기기 및 구글 \\n어시스턴트를 적용한 구글 픽셀(Pixel) 과 경쟁할 것으로 예상\\n☞ 출처 : 삼성전자 , ‘삼성 AI 포럼’서 자체 개발 생성형 AI ‘삼성 가우스 ’ 공개, 2023.11.08.\\n삼성전자 , ‘삼성 개발자 콘퍼런스 코리아 2023’ 개최, 2023.11.14.\\nTechRepublic, Samsung Gauss: Samsung Research Reveals Generative AI, 2023.11.08.\\n최적화된 크기의 모델 선택이 가능\\n∙삼성 가우스는 라이선스나 개인정보를 침해하지 않는 안전한 데이터를 통해 학습되었으며 , \\n온디바이스에서 작동하도록 설계되어 외부로 사용자의 정보가 유출되지 않는 장점을 보유\\n∙삼성전자는 삼성 가우스를 활용한 온디바이스 AI 기술도 소개했으며 , 생성 AI 모델을 다양한 제품에 \\n단계적으로 탑재할 계획\\nn삼성 가우스는 △텍스트를 생성하는 언어모델 △코드를 생성하는 코드 모델 △이미지를 생성하는  \\n이미지 모델의 3개 모델로 구성\\n∙언어 모델은 클라우드와 온디바이스 대상 다양한 모델로 구성되며 , 메일 작성, 문서 요약, 번역 업무의 \\n처리를 지원\\n∙코드 모델 기반의 AI 코딩 어시스턴트 ‘코드아이 (code.i)’ 는 대화형 인터페이스로 서비스를 제공하며 \\n사내 소프트웨어 개발에 최적화\\n∙이미지 모델은 창의적인 이미지를 생성하고 기존 이미지를 원하는 대로 바꿀 수 있도록 지원하며 \\n저해상도 이미지의 고해상도 전환도 지원\\n▹ 알리바바 클라우드 , 최신 LLM ‘통이치엔원 2.0’ 공개 ······················································ 9\\n   ▹ 삼성전자 , 자체 개발 생성 AI ‘삼성 가우스 ’ 공개 ··························································· 10\\n   ▹ 구글, 앤스로픽에 20억 달러 투자로 생성 AI 협력 강화 ················································ 11\\n   ▹ IDC, 2027 년 AI 소프트웨어 매출 2,500 억 달러 돌파 전망··········································· 12\\n   ▹ 빌 게이츠 , AI 에이전트로 인한 컴퓨터 사용의 패러다임 변화 전망································ 13',\n",
       " 'answer': {'query': '삼성전자가 자체 개발한 생성형 AI의 이름은 무엇인가요?',\n",
       "  'result': \"삼성전자가 자체 개발한 생성형 AI의 이름은 '삼성 가우스'입니다.\"},\n",
       " 'query': '삼성전자가 자체 개발한 생성형 AI의 이름은 무엇인가요?'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 함수 실행\n",
    "context_answer_rag_answer(\n",
    "    {\"question\": \"삼성전자가 자체 개발한 생성형 AI의 이름은 무엇인가요?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c08dbf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a teacher grading a quiz.\n",
      "You are given a question, the context the question is about, and the student's answer. You are asked to score the student's answer as either CORRECT or INCORRECT, based on the context.\n",
      "\n",
      "Example Format:\n",
      "QUESTION: question here\n",
      "CONTEXT: context the question is about here\n",
      "STUDENT ANSWER: student's answer here\n",
      "GRADE: CORRECT or INCORRECT here\n",
      "\n",
      "Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \n",
      "\n",
      "QUESTION: \u001b[33;1m\u001b[1;3m{query}\u001b[0m\n",
      "CONTEXT: \u001b[33;1m\u001b[1;3m{context}\u001b[0m\n",
      "STUDENT ANSWER: \u001b[33;1m\u001b[1;3m{result}\u001b[0m\n",
      "GRADE:\n"
     ]
    }
   ],
   "source": [
    "# cot_qa 평가자 생성\n",
    "cot_qa_evaluator = LangChainStringEvaluator(\n",
    "    \"cot_qa\",\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"answer\"],  # LLM 이 생성한 답변\n",
    "        \"reference\": run.outputs[\"context\"],  # Context\n",
    "        \"input\": example.inputs[\"question\"],  # 데이터셋의 질문\n",
    "    },\n",
    ")\n",
    "\n",
    "# context_qa 평가자 생성\n",
    "context_qa_evaluator = LangChainStringEvaluator(\n",
    "    \"context_qa\",\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"answer\"],  # LLM 이 생성한 답변\n",
    "        \"reference\": run.outputs[\"context\"],  # Context\n",
    "        \"input\": example.inputs[\"question\"],  # 데이터셋의 질문\n",
    "    },\n",
    ")\n",
    "\n",
    "# evaluator prompt 출력\n",
    "print_evaluator_prompt(context_qa_evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd6f08c",
   "metadata": {},
   "source": [
    "평가를 진행하고, 출력된 URL 로 이동하여 결과를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6457fd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'RAG_EVAL-a00e29f2' at:\n",
      "https://smith.langchain.com/o/d898bfb2-ece6-4cd6-ac84-3a348a8c5546/datasets/c60820b3-16d2-4901-8dc1-fbde819f2286/compare?selectedSessions=f46d0443-40a9-4ad0-acdf-4fb4432952be\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:10,  2.03s/it]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.context</th>\n",
       "      <th>outputs.answer</th>\n",
       "      <th>outputs.query</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.answer</th>\n",
       "      <th>feedback.COT Contextual Accuracy</th>\n",
       "      <th>feedback.Contextual Accuracy</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>삼성전자가 만든 생성형 AI의 이름은 무엇인가요?</td>\n",
       "      <td>사내 소프트웨어 개발에 최적화\\n∙이미지 모델은 창의적인 이미지를 생성하고 기존 이...</td>\n",
       "      <td>{'query': '삼성전자가 만든 생성형 AI의 이름은 무엇인가요?', 'resu...</td>\n",
       "      <td>삼성전자가 만든 생성형 AI의 이름은 무엇인가요?</td>\n",
       "      <td>None</td>\n",
       "      <td>삼성전자가 만든 생성형 AI의 이름은 테디노트 입니다.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.334567</td>\n",
       "      <td>2e4e6b01-4a0d-4f7e-abf8-5d2ddd02549e</td>\n",
       "      <td>5b5feec0-b77c-4476-b52d-45a95fa00338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>구글이 테디노트에게 20억달러를 투자한 것이 사실입니까?</td>\n",
       "      <td>1. 정책/법제  2. 기업/산업 3. 기술/연구  4. 인력/교육\\n구글, 앤스로...</td>\n",
       "      <td>{'query': '구글이 테디노트에게 20억달러를 투자한 것이 사실입니까?', '...</td>\n",
       "      <td>구글이 테디노트에게 20억달러를 투자한 것이 사실입니까?</td>\n",
       "      <td>None</td>\n",
       "      <td>사실이 아닙니다. 구글은 앤스로픽에 최대 20억 달러를 투자하기로 합의했으며, 이 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.581339</td>\n",
       "      <td>62a970ef-bb9d-4753-8776-5091855bc760</td>\n",
       "      <td>873b92e2-481f-4108-ac44-e1cbb77a5ced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>삼성전자가 만든 생성형 AI의 이름은 무엇인가요?</td>\n",
       "      <td>사내 소프트웨어 개발에 최적화\\n∙이미지 모델은 창의적인 이미지를 생성하고 기존 이...</td>\n",
       "      <td>{'query': '삼성전자가 만든 생성형 AI의 이름은 무엇인가요?', 'resu...</td>\n",
       "      <td>삼성전자가 만든 생성형 AI의 이름은 무엇인가요?</td>\n",
       "      <td>None</td>\n",
       "      <td>삼성전자가 만든 생성형 AI의 이름은 삼성 가우스 입니다.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.661615</td>\n",
       "      <td>cf9da67b-b28a-4864-a0bf-24656b5fb69d</td>\n",
       "      <td>a8315a79-2df6-4bbb-a191-b32a22f9533f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>미국 바이든 대통령이 안전하고 신뢰할 수 있는 AI 개발과 사용을 보장하기 위한 행...</td>\n",
       "      <td>1. 정책/법제  2. 기업/산업 3. 기술/연구  4. 인력/교육\\n미국, 안전하...</td>\n",
       "      <td>{'query': '미국 바이든 대통령이 안전하고 신뢰할 수 있는 AI 개발과 사용...</td>\n",
       "      <td>미국 바이든 대통령이 안전하고 신뢰할 수 있는 AI 개발과 사용을 보장하기 위한 행...</td>\n",
       "      <td>None</td>\n",
       "      <td>2023년 10월 30일 미국 바이든 대통령이 행정명령을 발표했습니다.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.462158</td>\n",
       "      <td>095fef54-943d-4c5d-8ad8-cdd8801a249c</td>\n",
       "      <td>9fff04ea-c365-448f-aab4-6370cb193d16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>코히어의 데이터 출처 탐색기에 대해서 간략히 말해주세요.</td>\n",
       "      <td>SPRi AI Brief |  \\n2023-12 월호\\n8코히어 , 데이터 투명성 ...</td>\n",
       "      <td>{'query': '코히어의 데이터 출처 탐색기에 대해서 간략히 말해주세요.', '...</td>\n",
       "      <td>코히어의 데이터 출처 탐색기에 대해서 간략히 말해주세요.</td>\n",
       "      <td>None</td>\n",
       "      <td>코히어의 데이터 출처 탐색기는 AI 모델 훈련에 사용되는 데이터셋의 출처와 라이선스...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.620755</td>\n",
       "      <td>28ec48fb-0155-45f1-8c7c-fb43f7615143</td>\n",
       "      <td>36349e56-96c3-44df-ab11-64262f171a7e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults RAG_EVAL-a00e29f2>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋 이름 설정\n",
    "dataset_name = \"RAG_EVAL_DATASET\"\n",
    "\n",
    "# 평가 실행\n",
    "evaluate(\n",
    "    context_answer_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=[cot_qa_evaluator, context_qa_evaluator],\n",
    "    experiment_prefix=\"RAG_EVAL\",\n",
    "    metadata={\n",
    "        \"variant\": \"COT_QA & Context_QA Evaluator 를 활용한 평가\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59560d72",
   "metadata": {},
   "source": [
    "![](./assets/output-02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a56015",
   "metadata": {},
   "source": [
    "평가 결과 `Ground Truth` 와 맞지 않은 답변을 생성해도 주어진 `Context` 가 맞다면 **CORRECT** 로 평가됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857bb208",
   "metadata": {},
   "source": [
    "## Criteria\n",
    "\n",
    "기준값 참조 레이블(정답 답변)이 없거나 얻기 힘든 경우 `\"criteria\"` 또는 `\"score\"` 평가자를 사용하여 사용자 지정 기준 집합에 대해 실행을 평가할 수 있습니다. \n",
    "\n",
    "이는 모델의 답변에 대한 **높은 수준의 의미론적 측면을 모니터링** 하려는 경우에 유용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4fc686",
   "metadata": {},
   "source": [
    "LangChainStringEvaluator(\"criteria\", config={ \"criteria\": `아래 중 하나의 criterion` })\n",
    "\n",
    "| 기준 | 설명 |\n",
    "|------|------|\n",
    "| `conciseness` | 답변이 간결하고 간단한지 평가 |\n",
    "| `relevance` | 답변이 질문과 관련이 있는지 평가 |\n",
    "| `correctness` | 답변이 옳은지 평가 |\n",
    "| `coherence` | 답변이 일관성이 있는지 평가 |\n",
    "| `harmfulness` | 답변이 해롭거나 유해한지 평가 |\n",
    "| `maliciousness` | 답변이 악의적이거나 악화시키는지 평가 |\n",
    "| `helpfulness` | 답변이 도움이 되는지 평가 |\n",
    "| `controversiality` | 답변이 논란이 되는지 평가 |\n",
    "| `misogyny` | 답변이 여성을 비하하는지 평가 |\n",
    "| `criminality` | 답변이 범죄를 촉진하는지 평가 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0b15f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "# 평가자 설정\n",
    "criteria_evaluator = [\n",
    "    LangChainStringEvaluator(\"criteria\", config={\"criteria\": \"conciseness\"}),\n",
    "    LangChainStringEvaluator(\"criteria\", config={\"criteria\": \"misogyny\"}),\n",
    "    LangChainStringEvaluator(\"criteria\", config={\"criteria\": \"criminality\"}),\n",
    "]\n",
    "\n",
    "# 데이터셋 이름 설정\n",
    "dataset_name = \"RAG_EVAL_DATASET\"\n",
    "\n",
    "# 평가 실행\n",
    "experiment_results = evaluate(\n",
    "    ask_question,\n",
    "    data=dataset_name,\n",
    "    evaluators=criteria_evaluator,\n",
    "    experiment_prefix=\"CRITERIA-EVAL\",\n",
    "    # 실험 메타데이터 지정\n",
    "    metadata={\n",
    "        \"variant\": \"criteria 를 활용한 평가\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a34bbf",
   "metadata": {},
   "source": [
    "![](./assets/output-03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f562976",
   "metadata": {},
   "source": [
    "## 정답이 존재하는 경우 Evaluator 활용(labeled_criteria)\n",
    "\n",
    "정답이 존재하는 경우, LLM 이 생성한 답변과 정답 답변을 비교하여 평가가 가능합니다.\n",
    "\n",
    "아래의 예시처럼 `reference` 에는 정답 답변을, `prediction` 에는 LLM 이 생성한 답변을 전달합니다.\n",
    "\n",
    "이 처럼 별도의 설정은 `prepare_data` 를 통해 정의합니다.\n",
    "\n",
    "또한, 답변 평가에 활용되는 LLM 은 `config` 의 `llm` 을 통해 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2abc8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# labeled_criteria 평가자 생성\n",
    "labeled_criteria_evaluator = LangChainStringEvaluator(\n",
    "    \"labeled_criteria\",\n",
    "    config={\n",
    "        \"criteria\": {\n",
    "            \"helpfulness\": (\n",
    "                \"Is this submission helpful to the user,\"\n",
    "                \" taking into account the correct reference answer?\"\n",
    "            )\n",
    "        },\n",
    "        \"llm\": ChatOpenAI(temperature=0.0, model=\"gpt-4o-mini\"),\n",
    "    },\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"answer\"],\n",
    "        \"reference\": example.outputs[\"answer\"],  # 정답 답변\n",
    "        \"input\": example.inputs[\"question\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "# evaluator prompt 출력\n",
    "print_evaluator_prompt(labeled_criteria_evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bde4831",
   "metadata": {},
   "source": [
    "아래는 `relevance` 를 평가하는 예시입니다.\n",
    "\n",
    "이번에는 `prepare_data` 를 통해 `reference` 를 `context` 로 전달합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa4de48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "relevance_evaluator = LangChainStringEvaluator(\n",
    "    \"labeled_criteria\",\n",
    "    config={\n",
    "        \"criteria\": \"relevance\",\n",
    "        \"llm\": ChatOpenAI(temperature=0.0, model=\"gpt-4o-mini\"),\n",
    "    },\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"answer\"],\n",
    "        \"reference\": run.outputs[\"context\"],  # Context 를 전달\n",
    "        \"input\": example.inputs[\"question\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "print_evaluator_prompt(relevance_evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbe0e61",
   "metadata": {},
   "source": [
    "평가를 진행하고, 출력된 URL 로 이동하여 결과를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee047abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "# 데이터셋 이름 설정\n",
    "dataset_name = \"RAG_EVAL_DATASET\"\n",
    "\n",
    "# 평가 실행\n",
    "experiment_results = evaluate(\n",
    "    context_answer_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=[labeled_criteria_evaluator, relevance_evaluator],\n",
    "    experiment_prefix=\"LABELED-EVAL\",\n",
    "    # 실험 메타데이터 지정\n",
    "    metadata={\n",
    "        \"variant\": \"labeled_criteria evaluator 활용한 평가\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a657acc",
   "metadata": {},
   "source": [
    "![](./assets/output-04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f44c6f",
   "metadata": {},
   "source": [
    "## 사용자 정의 점수 Evaluator(labeled_score_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663e2129",
   "metadata": {},
   "source": [
    "아래는 점수를 반환하는 평가자 생성 예시입니다. `normalize_by` 를 통해 점수를 정규화할 수 있습니다. 변환된 점수는 (0 ~ 1) 사이의 값으로 정규화됩니다.\n",
    "\n",
    "아래의 `accuracy` 는 사용자가 임의로 정의한 기준입니다. 적합한 Prompt 를 정의하여 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b193f4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator\n",
    "\n",
    "# 점수를 반환하는 평가자 생성\n",
    "labeled_score_evaluator = LangChainStringEvaluator(\n",
    "    \"labeled_score_string\",\n",
    "    config={\n",
    "        \"criteria\": {\n",
    "            \"accuracy\": \"How accurate is this prediction compared to the reference on a scale of 1-10?\"\n",
    "        },\n",
    "        \"normalize_by\": 10,\n",
    "        \"llm\": ChatOpenAI(temperature=0.0, model=\"gpt-4o-mini\"),\n",
    "    },\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"answer\"],\n",
    "        \"reference\": example.outputs[\"answer\"],\n",
    "        \"input\": example.inputs[\"question\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "print_evaluator_prompt(labeled_score_evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6644a6cf",
   "metadata": {},
   "source": [
    "평가를 진행하고, 출력된 URL 로 이동하여 결과를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5671567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "# 평가 실행\n",
    "experiment_results = evaluate(\n",
    "    ask_question,\n",
    "    data=dataset_name,\n",
    "    evaluators=[labeled_score_evaluator],\n",
    "    experiment_prefix=\"LABELED-SCORE-EVAL\",\n",
    "    # 실험 메타데이터 지정\n",
    "    metadata={\n",
    "        \"variant\": \"labeled_score 활용한 평가\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9781c490",
   "metadata": {},
   "source": [
    "![](./assets/output-05.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
