{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain of Density: https://arxiv.org/pdf/2309.04269.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import textwrap\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "# Load some data to summarize\n",
    "loader = WebBaseLoader(\"https://teddylee777.github.io/data-science/optuna/\")\n",
    "docs = loader.load()\n",
    "content = docs[0].page_content\n",
    "\n",
    "# Get this prompt template\n",
    "prompt = hub.pull(\"lawwu/chain_of_density\")\n",
    "\n",
    "# The chat model output is a JSON list of dicts, with SimpleJsonOutputParser\n",
    "# we can convert it o a dict, and it suppors streaming.\n",
    "json_parser = SimpleJsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"ARTICLE\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0.1)\n",
    "    | json_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Missing_Entities': 'Optuna',\n",
       "  'Denser_Summary': \"Optuna is a hyperparameter optimization framework; it offers suggest_categorical, suggest_int, and suggest_float methods. Optuna's objective is to minimize the error using 5-Fold Cross Validation.\"},\n",
       " {'Missing_Entities': 'objective function',\n",
       "  'Denser_Summary': 'The objective function in Optuna aims to minimize error through 5-Fold Cross Validation. It utilizes suggest_categorical, suggest_int, and suggest_float methods for parameter optimization.'},\n",
       " {'Missing_Entities': 'RandomForestRegressor',\n",
       "  'Denser_Summary': 'The RandomForestRegressor model is used in the Optuna optimization process. The objective function minimizes error using 5-Fold Cross Validation with RandomForestRegressor.'},\n",
       " {'Missing_Entities': 'study.best_params',\n",
       "  'Denser_Summary': 'The best_params from the study in Optuna are printed. The RandomForestRegressor model is instantiated with the best_params for training.'},\n",
       " {'Missing_Entities': 'mean_squared_error',\n",
       "  'Denser_Summary': 'The mean_squared_error metric is used in the objective function. The RandomForestRegressor model is trained using the best_params obtained from Optuna.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "[\n",
      "    {\n",
      "        \"Missing_Entities\": \"\",\n",
      "        \"Denser_Summary\": \"이 기사는 데이터사이언스, 머신러닝, 인공지능에 대한 개념을 설명하고 있습니다. 박정현 서울대 EPM 연구원이 작성한 이 글에서는 이 세 분야가 우리 생활에 어떻게 적용되고 있는지, 그리고 이 분야들을 공부하고자 하는 사람들을 위한 가이드를 제공하고 있습니다. 또한, 데이터사이언스가 가장 넓은 범위를 가지고 있으며, 머신러닝과 인공지능이 어떻게 서로 다른지에 대해서도 설명하고 있습니다. 이 글은 이 분야에 대한 기본적인 이해를 돕기 위해 여러 대학에서 데이터사이언스와 인공지능 대학원을 설립하는 등의 교육적 변화에 대해서도 언급하고 있습니다.\"\n",
      "    },\n",
      "    {\n",
      "        \"Missing_Entities\": \"튜링테스트; 지도학습(supervised learning); 비지도 학습(unsupervised learning)\",\n",
      "        \"Denser_Summary\": \"박정현 서울대 EPM 연구원이 작성한 이 글에서는 데이터사이언스, 머신러닝, 인공지능의 개념과 실생활 적용 사례를 설명합니다. 데이터사이언스가 가장 넓은 범위를 가지며, 인공지능의 이해를 위해 튜링테스트를 언급하고, 머신러닝을 지도학습과 비지도 학습으로 구분하여 설명합니다. 또한, 이 분야의 교육적 변화와 대학에서의 데이터사이언스 및 인공지능 대학원 설립에 대해서도 언급하며, 이 분야에 대한 기본적인 이해를 돕습니다.\"\n",
      "    },\n",
      "    {\n",
      "        \"Missing_Entities\": \"알파고; GPT-3; 빅데이터; 통계적 기법\",\n",
      "        \"Denser_Summary\": \"박정현 서울대 EPM 연구원이 작성한 글에서는 데이터사이언스, 머신러닝, 인공지능의 정의와 알파고, GPT-3 같은 실생활 적용 사례를 소개합니다. 데이터사이언스는 빅데이터와 통계적 기법을 기반으로 하며, 인공지능의 이해를 위한 튜링테스트, 머신러닝의 지도학습 및 비지도 학습 구분을 설명합니다. 또한, 교육적 변화와 대학원 설립에 대한 정보를 제공하며, 이 분야의 기본적인 이해를 돕습니다.\"\n",
      "    },\n",
      "    {\n",
      "        \"Missing_Entities\": \"서울대 EPM연구원; 룰 기반(rule-based) 시스템; 머신러닝 기반(machine learning) 시스템\",\n",
      "        \"Denser_Summary\": \"서울대 EPM연구원 박정현이 작성한 글에서는 데이터사이언스, 머신러닝, 인공지능의 정의와 알파고, GPT-3 등의 적용 사례를 소개합니다. 데이터사이언스는 빅데이터와 통계적 기법을 기반으로, 인공지능은 튜링테스트로 설명되며, 머신러닝은 지도학습과 비지도 학습으로 구분됩니다. 이 글은 또한 룰 기반과 머신러닝 기반 시스템의 차이를 설명하고, 교육적 변화와 대학원 설립에 대한 정보를 제공합니다.\"\n",
      "    },\n",
      "    {\n",
      "        \"Missing_Entities\": \"이메일 스팸 필터링; k-means; 이상값 검출(anomaly detection); 강화학습(reinforce learning)\",\n",
      "        \"Denser_Summary\": \"서울대 EPM연구원 박정현의 글은 데이터사이언스, 머신러닝, 인공지능의 정의와 알파고, GPT-3 등의 적용 사례를 다룹니다. 데이터사이언스는 빅데이터와 통계적 기법을 기반으로 하며, 인공지능은 튜링테스트로, 머신러닝은 지도학습, 비지도 학습으로 구분됩니다. 이 글은 이메일 스팸 필터링, k-means, 이상값 검출, 강화학습 등 머신러닝의 적용을 설명하고, 교육적 변화와 대학원 설립에 대한 정보를 제공합니다.\"\n",
      "    }\n",
      "]\n",
      "```서울대 EPM연구원 박정현의 글은 데이터사이언스, 머신러닝, 인공지능의 정의와 알파고, GPT-3 등의 적용 사례를 다룹니다. 데이터사이언스는 빅데이터와 통계적 기법을 기반으로 하며, 인공지능은 튜링테스트로, 머신러닝은 지도학습, 비지도 학습으로 구분됩니다. 이 글은 이메일 스팸 필터링, k-means, 이상값 검출, 강화학습 등 머신러닝의 적용을 설명하고, 교육적 변화와 대학원 설립에 대한 정보를 제공합니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "import json\n",
    "\n",
    "\n",
    "# Load some data to summarize\n",
    "loader = WebBaseLoader(\n",
    "    \"https://www.aitimes.com/news/articleView.html?idxno=131777\")\n",
    "docs = loader.load()\n",
    "content = docs[0].page_content\n",
    "# Load the prompt\n",
    "# prompt = hub.pull(\"langchain-ai/chain-of-density:4f55305e\")\n",
    "\n",
    "\n",
    "class StreamCallback(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token, **kwargs):\n",
    "        print(token, end=\"\", flush=True)\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Article: {ARTICLE}\n",
    "You will generate increasingly concise, entity-dense summaries of the above article. \n",
    "\n",
    "Repeat the following 2 steps 5 times. \n",
    "\n",
    "Step 1. Identify 1-3 informative entities (\";\" delimited) from the article which are missing from the previously generated summary. \n",
    "Step 2. Write a new, denser summary of identical length which covers every entity and detail from the previous summary plus the missing entities. \n",
    "\n",
    "A missing entity is:\n",
    "- relevant to the main story, \n",
    "- specific yet concise (50 words or fewer), \n",
    "- novel (not in the previous summary), \n",
    "- faithful (present in the article), \n",
    "- anywhere (can be located anywhere in the article).\n",
    "\n",
    "Guidelines:\n",
    "\n",
    "- The first summary should be long (8-10 sentences, ~200 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose language and fillers (e.g., \"this article discusses\") to reach ~200 words.\n",
    "- Make every word count: rewrite the previous summary to improve flow and make space for additional entities.\n",
    "- Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".\n",
    "- The summaries should become highly dense and concise yet self-contained, i.e., easily understood without the article. \n",
    "- Missing entities can appear anywhere in the new summary.\n",
    "- Never drop entities from the previous summary. If space cannot be made, add fewer new entities. \n",
    "\n",
    "Remember, use the exact same number of words for each summary.\n",
    "Answer in JSON. The JSON should be a list (length 5) of dictionaries whose keys are \"Missing_Entities\" and \"Denser_Summary\".\n",
    "Use only KOREAN language to reply.\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# Create the chain, including\n",
    "chain = (\n",
    "    prompt\n",
    "    | ChatOpenAI(\n",
    "        temperature=0,\n",
    "        model=\"gpt-4-turbo-preview\",\n",
    "        streaming=True,\n",
    "        callbacks=[StreamCallback()],\n",
    "    )\n",
    "    | JsonOutputParser()\n",
    "    | (lambda x: x[-1][\"Denser_Summary\"])\n",
    ")\n",
    "\n",
    "# Invoke the chain\n",
    "result = chain.invoke({\"ARTICLE\": content})\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
