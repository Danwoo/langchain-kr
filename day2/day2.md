# DAY2

## 1. 대표적인 벡터 데이터베이스(Vector Database)

| 구분         | Faiss (Facebook AI Similarity Search) | Chroma (Chroma DB) | Pinecone |
|--------------|---------------------------------------|--------------------|----------|
| **정의**     | 고속 유사도 검색 라이브러리 📚         | 개발자 친화적인 오픈소스 벡터 데이터베이스 💻 | 완전 관리형(Fully-managed) 클라우드 벡터 데이터베이스 ☁️ |
| **형태**     | 오픈소스 라이브러리                    | 오픈소스 데이터베이스 | 상용 SaaS (서비스형 소프트웨어) |
| **장점**     | ✅ 압도적인 검색 속도(GPU 활용)<br>✅ 높은 유연성 및 제어<br>✅ 무료(오픈소스) | ✅ 개발 편의성(간단한 설치/사용)<br>✅ 오픈소스(직접 수정/확장 가능)<br>✅ 메타데이터 필터링 지원 | ✅ 운영 부담 없음(완전 관리형)<br>✅ 뛰어난 확장성<br>✅ 실시간 데이터 처리 |
| **단점**     | ❌ 높은 학습 곡선<br>❌ 운영 복잡성<br>❌ 메타데이터 필터링 제한 | ❌ 대규모 환경 한계<br>❌ 직접 운영 필요<br>❌ 작은 커뮤니티 | ❌ 비용 발생(사용량 기반)<br>❌ 낮은 유연성<br>❌ 온프레미스 불가 |
| **주요 사용 사례** | 🔹 연구 및 개발 환경<br>🔹 대규모 서비스의 극한 검색 성능<br>🔹 시스템 완벽 제어 필요 시 | 🔹 빠른 프로토타입 개발<br>🔹 개인/소규모 프로젝트<br>🔹 LLM 프레임워크(LangChain 등) 연동 | 🔹 인프라 관리 최소화<br>🔹 대규모 상용 AI 서비스<br>🔹 안정적/확장성 높은 운영 필요 시 |


## 2. LangChain에서 가장 많이 사용하는 검색기(Retriever) 2가지 및 실제 적용 사례
LangChain은 대규모 언어 모델(LLM)을 기반으로 강력한 애플리케이션을 구축하기 위한 핵심 프레임워크입니다. 그중에서도 '검색기(Retriever)'는 외부 데이터 소스에서 정보를 가져와 LLM의 답변을 더욱 풍부하고 정확하게 만드는 데 결정적인 역할을 합니다.    
수많은 검색기 중에서 현재 가장 널리 사용되고 대표적인 2가지 검색기와 그 실제 적용 사례를 소개합니다.

### 1. Vector Store Retriever (벡터 저장소 검색기)   
가장 기본적이고 핵심적인 검색기   
Vector Store Retriever는 LangChain에서 가장 보편적으로 사용되는 검색기입니다. 사용자의 질문이나 텍스트를 벡터(Vector)로 변환하고, 미리 벡터화하여 저장해 둔 문서 데이터베이스에서 의미적으로 가장 유사한 문서를 찾아내는 방식으로 작동합니다. 이는 단순한 키워드 매칭을 넘어 문맥적 의미를 파악하여 관련성 높은 정보를 검색할 수 있게 해줍니다.

주요 특징:   
의미 기반 검색: 키워드가 일치하지 않더라도 의미적으로 유사한 내용을 검색하여 더 정확한 결과를 제공합니다.   
다양한 벡터 DB 호환: FAISS, Chroma, Pinecone 등 다양한 벡터 데이터베이스와 쉽게 연동하여 사용할 수 있습니다.   
RAG의 핵심: LLM이 학습하지 않은 최신 정보나 특정 도메인 지식(예: 사내 문서)을 답변에 활용하는 RAG(Retrieval-Augmented Generation) 아키텍처의 기반이 됩니다.   

실제 적용 사례: A사의 고객 지원 챗봇   
A사와 같은 이커머스 기업은 매일 수많은 고객 문의에 대응해야 합니다.   
모든 상담원이 모든 제품 정보, 배송 정책, 환불 규정을 완벽하게 숙지하기는 어렵습니다.   

문제점: 고객 문의에 대한 상담원의 답변 속도가 느리고, 정보의 일관성이 떨어져 고객 만족도가 저하되었습니다.   
해결책: 
1) 모든 제품 정보, FAQ, 내부 정책 문서를 텍스트로 추출하여 임베딩 모델을 통해 벡터로 변환합니다.   
2) 변환된 벡터들을 FAISS와 같은 벡터 데이터베이스에 저장합니다.   
3) 고객의 질문이 챗봇에 입력되면, LangChain의 Vector Store Retriever가 해당 질문을 벡터로 변환하여 벡터 DB에서 가장 유사한 정책이나 제품 정보 문서를 신속하게 찾아냅니다.
4) 검색된 문서를 기반으로 LLM이 최종 답변을 생성하여 고객에게 제공합니다.   
결과: 24시간 운영되는 자동화된 챗봇을 통해 고객 문의에 즉각적이고 일관된 답변을 제공할 수 있게 되었습니다. 이를 통해 상담원의 업무 부담을 줄이고, 고객 만족도를 크게 향상시켰습니다.

### 2. Ensemble Retriever (앙상블 검색기)   
정확도를 극대화하는 하이브리드 검색   
Ensemble Retriever는 두 가지 이상의 검색기를 결합하여 각 검색기의 장점을 취하고 단점을 보완하는, 보다 발전된 형태의 검색기입니다. 가장 일반적인 조합은 키워드 기반의 전통적인 검색 방식(Sparse Retriever, 예: BM25)과 의미 기반의 벡터 검색(Dense Retriever, 예: Vector Store Retriever)을 함께 사용하는 '하이브리드 검색(Hybrid Search)'입니다.

주요 특징:   
검색 정확도 향상: 키워드가 명확한 경우에는 BM25가, 문맥적 이해가 중요한 경우에는 벡터 검색이 우수한 성능을 보입니다. Ensemble Retriever는 이 둘을 결합하여 어떤 유형의 질문에도 안정적으로 높은 검색 품질을 제공합니다.

결과 재조정: 각 검색기로부터 받은 결과를 'Reciprocal Rank Fusion'과 같은 알고리즘을 사용해 지능적으로 재조정하여 최종 순위를 매깁니다.
유연한 조합: 필요에 따라 다양한 종류와 수의 검색기를 조합하여 맞춤형 검색 시스템을 구축할 수 있습니다.

실제 적용 사례: B 법률 사무소의 법률 정보 검색 시스템
법률 분야에서는 특정 법률 용어나 판례 번호와 같은 '키워드' 검색의 정확성과 함께, 복잡한 법적 상황에 대한 '의미' 기반의 검색이 모두 중요합니다.
문제점: 변호사들이 방대한 법률 문서와 판례 데이터베이스에서 필요한 정보를 찾는 데 많은 시간을 소모했습니다. 키워드 검색만으로는 관련된 모든 판례를 찾기 어렵고, 의미 검색만으로는 특정 용어가 포함된 핵심 문서를 놓칠 수 있었습니다.

해결책:   
1) 법률 용어와 판례 번호를 정확하게 찾아내기 위한 BM25 Retriever를 설정합니다.   
2) 판례의 내용과 법률 해석 등 의미적 유사성을 기반으로 문서를 찾기 위한 Vector Store Retriever를 구축합니다.   
3) LangChain의 Ensemble Retriever를 사용하여 이 두 검색기를 하나로 묶어 하이브리드 검색 시스템을 구현합니다.   

결과: 변호사가 "부동산 사기 관련 최근 5년 판례"와 같이 복합적인 질문을 입력하면, Ensemble Retriever가 키워드 검색과 의미 검색을 동시에 수행하여 가장 관련성 높은 판례와 법률 문서를 종합적으로 추천해 줍니다.   
이를 통해 리서치 시간을 획기적으로 단축하고, 더욱 정확한 법률 자문을 제공할 수 있게 되었습니다.


## 3. RRF와 CC방식의 차이점
앙상블 리트리버(Ensemble Retriever)에서 사용되는 RRF와 CC 방식의 가장 큰 차이점은 처리 대상과 목적에 있습니다.   
간단히 말해, **RRF(Reciprocal Rank Fusion)**는 여러 검색 결과의 **'순위'**를 합쳐 최종 문서 목록을 만드는 랭킹 알고리즘이고, **CC(Contextual Compression)**는 검색된 문서의 **'내용'**을 요약하여 관련성 높은 부분만 남기는 후처리 기술입니다.

### 1. RRF (Reciprocal Rank Fusion): 순위 기반 랭킹 퓨전 🏅
RRF는 앙상블 리트리버의 핵심적인 순위 재조정(Re-ranking) 알고리즘입니다.   
목적: 여러 검색기(예: 키워드 기반 BM25, 의미 기반 벡터 검색)가 각각 반환한 검색 결과 목록들을 공정하게 합쳐, 가장 종합적으로 관련성이 높은 순서로 최종 목록을 만드는 것입니다.

작동 방식: 각 검색기가 반환한 결과 목록에서 문서의 **순위(rank)**를 확인합니다.   
문서의 내용은 보지 않고, 오직 이 순위 정보에만 기반하여 점수를 매깁니다. (일반적으로 순위가 높을수록 높은 점수를 받습니다.)
모든 검색기에서 받은 점수를 합산하여 최종 점수를 계산하고, 이 점수가 높은 순으로 문서 목록을 재정렬합니다.   
핵심: 어떤 문서를 사용자에게 보여줄지 그 '순서'를 결정하는 역할에 집중합니다.

### 2. CC (Contextual Compression): 내용 기반 압축 📝
CC(Contextual Compression)는 RRF와 같이 순위를 매기는 기술이 아니라, 검색된 문서의 내용을 효율적으로 가공하는 기술입니다.   
목적: 검색된 문서의 내용이 너무 길거나 질문과 관련 없는 정보가 많을 경우, LLM에 전달하기 전에 질문과 관련된 핵심 정보만 추출하여 노이즈를 줄이고 LLM의 답변 정확도를 높이는 것입니다.   
작동 방식:    
기본 리트리버(앙상블 리트리버 포함)가 먼저 문서 목록을 가져옵니다.   
그 다음, 각 문서의 전체 내용을 LLM이나 Cross-Encoder 같은 다른 모델에 보내 질문과의 관련성을 판단합니다.   
관련 있는 문장이나 구절만 남기고 나머지 불필요한 부분은 **제거(압축)**합니다.

핵심: 검색된 문서의 '내용'을 어떻게 가공하여 더 유용하게 만들지를 결정하는 역할입니다.

| 구분         | RRF (Reciprocal Rank Fusion)         | CC (Contextual Compression)      |
|--------------|--------------------------------------|----------------------------------|
| 역할         | 랭킹 알고리즘 (순위 결정)            | 후처리 기술 (내용 압축)           |
| 처리 대상    | 검색 결과 목록의 순위                | 검색된 문서의 내용                |
| 적용 단계    | 여러 검색 결과를 하나로 합칠 때      | 최종 문서 목록이 결정된 후에      |
| 결과물       | 순서가 재조정된 문서 목록            | 내용이 압축된 문서 목록            |


### 도서관 사서 비유 📚   
여러 명의 사서(각기 다른 검색기)가 각자 주제에 맞는 추천 도서 목록을 가져왔다고 상상해 보세요.   
RRF는 이 여러 목록을 비교해보고 어떤 책을 맨 위부터 보여줄지 최종 전시 순서를 정하는 수석 사서와 같습니다.   
CC는 전시 순서가 정해진 책들을 넘겨받아, 질문과 관련된 핵심 페이지만 요약해서 보여주는 편집자와 같습니다.   
실제 애플리케이션에서는 Ensemble Retriever (RRF 사용)로 최적의 문서 목록을 찾은 뒤, 그 결과를 Contextual Compression Retriever로 한 번 더 처리하여 LLM에 가장 정제된 정보를 전달하는 방식으로 함께 사용될 수 있습니다.